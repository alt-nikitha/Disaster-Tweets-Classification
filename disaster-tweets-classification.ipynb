{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Using CNNs to classify disaster tweets\nLet's look at the training data first. We can use the pandas module to read our training data and store it in a dataframe. When we view the first few rows of the dataframe, we can see that there are 5 columns, and 2 columns have missing values. We then drop these columns as they aren't really useful for classification. \nBefore we send data into the model, we need to convert it into a ....\n## Text Preprocessing\nSo we begin text preprocessing by converting the 'text' column of the dataframe into a list. Each tweet is an element of this list. Now, from each tweet we remove numbers using regex pattern matching."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nnp.random.seed(500) \ntrain=pd.DataFrame(pd.read_csv('../input/nlp-getting-started/train.csv'))\ntrain.head()\ntrain=train.dropna(axis=1)\ntweets=train['text'].to_list()\n# print(train)\n# print(tweets)\nnonums=[]\nfor tweet in tweets:\n    nonums.append(re.sub(r'\\d+', '', tweet))\n# print(nonums)","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When you print the output, you can see that there are a bunch of URLs in the tweets as well. We remove these by matching any text that begins with 'http\\'."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install contractions\np=re.compile(r'\\<http.+?\\>', re.DOTALL)\n\ntweetswithouturls=[]\nfor tweet in nonums:\n    tweetswithouturls.append(re.sub(r\"http\\S+\", \"\", tweet))\n# print(tweetswithouturls)","execution_count":4,"outputs":[{"output_type":"stream","text":"Collecting contractions\n  Downloading contractions-0.0.25-py2.py3-none-any.whl (3.2 kB)\nCollecting textsearch\n  Downloading textsearch-0.0.17-py2.py3-none-any.whl (7.5 kB)\nRequirement already satisfied: pyahocorasick in /opt/conda/lib/python3.7/site-packages (from textsearch->contractions) (1.4.0)\nRequirement already satisfied: Unidecode in /opt/conda/lib/python3.7/site-packages (from textsearch->contractions) (1.1.1)\nInstalling collected packages: textsearch, contractions\nSuccessfully installed contractions-0.0.25 textsearch-0.0.17\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Next, we replace contraced forms of words like 'don't' and 'can't' with their expanded forms 'do not' and 'cannot'."},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport contractions\ndef replace_contractions(text):\n    \"\"\"Replace contractions in string of text\"\"\"\n    return contractions.fix(text)\nnocontractions=[]\nfor tweet in tweetswithouturls:\n    \n\n    nocontractions.append(replace_contractions(tweet))\n# print(nocontractions)","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We then tokenise each tweet. With tokenising, we transform each tweet into a list, with each element of this tweet list being each word in the tweet. These words are called tokens."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\ntokens = [word_tokenize(sen) for sen in nocontractions]\n# print(tokens)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that some of the tokens are punctiuation indicators like  .  ,  ?  , ! . These aren't necessary as well, as we need to understand the text in terms of the words and their context only. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\nnopunct=[]\nfor listt in tokens:\n    nopunct.append(remove_punctuation(listt))\n# print(nopunct)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We then remove any characters which are not ascii characters. So we retain only A-Z as we removed numbers already."},{"metadata":{"trusted":true},"cell_type":"code","source":"import string, unicodedata\ndef remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        if(new_word.isalpha()):\n            new_words.append(new_word)\n    return new_words\nonlyascii=[]\nfor listt in nopunct:\n    onlyascii.append(remove_non_ascii(listt))\n# print(onlyascii)","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To maintain uniformity we convert all letters to lowercase."},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_lowercase(words):\n    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = word.lower()\n        new_words.append(new_word)\n    return new_words\nlower=[]\nfor listt in onlyascii:\n    lower.append(to_lowercase(listt))\n# print(lower)","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We then remove stopwords, which don't necessarily add to convey the main idea of the tweet. These include words like i, our, of, for."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef remove_stopwords(words):\n    \"\"\"Remove stop words from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        if word not in stopwords.words('english'):\n            new_words.append(word)\n    return new_words\n\n    \nnostopwords=[]\nfor listt in lower:\n    nostopwords.append(remove_stopwords(listt))\n# print(nostopwords)\n# print(stopwords.words('english'))","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we lemmatize the words to use only the root forms of the words, unless they're nouns."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet as wn\nimport collections\ntag_map = collections.defaultdict(lambda : wn.NOUN)\ntag_map['J'] = wn.ADJ\ntag_map['V'] = wn.VERB\ntag_map['R'] = wn.ADV\nFinal_words = []\nword_Lemmatized = WordNetLemmatizer()\nfor entry in nostopwords:\n#     print(entry)\n    words=[]\n    # Initializing WordNetLemmatizer()\n    \n    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n    for word, tag in pos_tag(entry):\n        # Below condition is to check for Stop words and consider only alphabets\n        \n        word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n#         print(word_Final)\n        words.append(word_Final)\n    Final_words.append(words)\n# print(Final_words)\n# for entry in nostopwords:\n#     print(pos_tag(entry))","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now add these preprocessed tokens next to the tweets in the dataframe. We add them as sentences or a string of tokens in one column called \"Text_Final\". Then in another column called \"tokens\", we add the them as a list of tokens. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Text_Final'] = [' '.join(sen) for sen in Final_words]\ntrain['tokens'] = Final_words\ndisaster = []\nnotdisaster = []\nfor l in train['target']:\n    if l == 0:\n        disaster.append(0)\n        notdisaster.append(1)\n    elif l == 1:\n        disaster.append(1)\n        notdisaster.append(0)\ntrain['Disaster']= disaster\ntrain['Not a Disaster']= notdisaster\n\ntrain = train[['Text_Final', 'tokens', 'target', 'Disaster', 'Not a Disaster']]\ntrain.head()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"                                          Text_Final  \\\n0         deed reason earthquake may allah forgive u   \n1              forest fire near la ronge sask canada   \n2  resident ask shelter place notify officer evac...   \n3  people receive wildfire evacuation order calif...   \n4  get sent photo ruby alaska smoke wildfires pou...   \n\n                                              tokens  target  Disaster  \\\n0  [deed, reason, earthquake, may, allah, forgive...       1         1   \n1      [forest, fire, near, la, ronge, sask, canada]       1         1   \n2  [resident, ask, shelter, place, notify, office...       1         1   \n3  [people, receive, wildfire, evacuation, order,...       1         1   \n4  [get, sent, photo, ruby, alaska, smoke, wildfi...       1         1   \n\n   Not a Disaster  \n0               0  \n1               0  \n2               0  \n3               0  \n4               0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text_Final</th>\n      <th>tokens</th>\n      <th>target</th>\n      <th>Disaster</th>\n      <th>Not a Disaster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>deed reason earthquake may allah forgive u</td>\n      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>forest fire near la ronge sask canada</td>\n      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>resident ask shelter place notify officer evac...</td>\n      <td>[resident, ask, shelter, place, notify, office...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>people receive wildfire evacuation order calif...</td>\n      <td>[people, receive, wildfire, evacuation, order,...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>get sent photo ruby alaska smoke wildfires pou...</td>\n      <td>[get, sent, photo, ruby, alaska, smoke, wildfi...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We then repeat all the processing steps for the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#repeating for test\ntest=pd.DataFrame(pd.read_csv('../input/nlp-getting-started/test.csv'))\ntest.head()\ntest=test.dropna(axis=1)\ntweetstest=test['text'].to_list()\n# print(train)\n# print(tweets)\nnonumstest=[]\nfor tweet in tweetstest:\n    nonumstest.append(re.sub(r'\\d+', '', tweet))\n# print(nonums)\n\n\ntweetswithouturlstest=[]\nfor tweet in nonumstest:\n    tweetswithouturlstest.append(re.sub(r\"http\\S+\", \"\", tweet))\n# print(tweetswithouturlstest)\n\nnocontractionstest=[]\nfor tweet in tweetswithouturlstest:\n    \n    nocontractionstest.append(replace_contractions(tweet))\n# print(nocontractionstest)\ntokenstest = [word_tokenize(sen) for sen in nocontractionstest]\n# print(tokenstest)\n\nnopuncttest=[]\nfor listt in tokenstest:\n    nopuncttest.append(remove_punctuation(listt))\n# print(nopuncttest)\n\nonlyasciitest=[]\nfor listt in nopuncttest:\n    onlyasciitest.append(remove_non_ascii(listt))\n# print(onlyasciitest)\n\nlowertest=[]\nfor listt in onlyasciitest:\n    lowertest.append(to_lowercase(listt))\n# print(lowertest)\n\nnostopwordstest=[]\nfor listt in lowertest:\n    nostopwordstest.append(remove_stopwords(listt))\n# print(nostopwordstest)\nFinalwordstest=[]\nfor entry in nostopwordstest:\n#     print(entry)\n    words=[]\n    # Initializing WordNetLemmatizer()\n    \n    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n    for word, tag in pos_tag(entry):\n        # Below condition is to check for Stop words and consider only alphabets\n        \n        word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n#         print(word_Final)\n        words.append(word_Final)\n    Finalwordstest.append(words)\n# print(Finalwordstest)","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we add the tokenised test tweets as sentences in one column called \"Text_Final\" and as tokens in another column called \"tokens\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Text_Final'] = [' '.join(sen) for sen in Finalwordstest]\ntest['tokens'] = Finalwordstest\ntest.head()","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"   id                                               text  \\\n0   0                 Just happened a terrible car crash   \n1   2  Heard about #earthquake is different cities, s...   \n2   3  there is a forest fire at spot pond, geese are...   \n3   9           Apocalypse lighting. #Spokane #wildfires   \n4  11      Typhoon Soudelor kills 28 in China and Taiwan   \n\n                                          Text_Final  \\\n0                          happen terrible car crash   \n1  heard earthquake different city stay safe ever...   \n2  forest fire spot pond geese flee across street...   \n3                  apocalypse light spokane wildfire   \n4                 typhoon soudelor kill china taiwan   \n\n                                              tokens  \n0                     [happen, terrible, car, crash]  \n1  [heard, earthquake, different, city, stay, saf...  \n2  [forest, fire, spot, pond, geese, flee, across...  \n3             [apocalypse, light, spokane, wildfire]  \n4           [typhoon, soudelor, kill, china, taiwan]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>Text_Final</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Just happened a terrible car crash</td>\n      <td>happen terrible car crash</td>\n      <td>[happen, terrible, car, crash]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n      <td>heard earthquake different city stay safe ever...</td>\n      <td>[heard, earthquake, different, city, stay, saf...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n      <td>forest fire spot pond geese flee across street...</td>\n      <td>[forest, fire, spot, pond, geese, flee, across...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n      <td>apocalypse light spokane wildfire</td>\n      <td>[apocalypse, light, spokane, wildfire]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n      <td>typhoon soudelor kill china taiwan</td>\n      <td>[typhoon, soudelor, kill, china, taiwan]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We then create a bag of training words as a list called \"all_training_words\". This contains all the words in all of the training tweets as one list. We create another array called \"training_sentences_length\" which stores a list of lengths of each tweet.\nThen a list called \"TRAINING_VOCAB\" is created to store the set of unique words in all of the tweets in the training set sorted alphabetically. So this gives us the vocabulary we're dealing with to train our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_training_words = [word for tokens in train[\"tokens\"] for word in tokens]\n\n# count_vect = CountVectorizer()\n# X_train_counts = count_vect.fit_transform(all_training_words)\n# print(X_train_counts)\ntraining_sentence_lengths = [len(tokens) for tokens in train[\"tokens\"]]\nTRAINING_VOCAB = sorted(list(set(all_training_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\nprint(\"Max sentence length is %s\" % max(training_sentence_lengths))\n### for svm split data\n\n\n","execution_count":34,"outputs":[{"output_type":"stream","text":"67863 words total, with a vocabulary size of 14247\nMax sentence length is 23\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We repeat the above process for test data as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_test_words = [word for tokens in test[\"tokens\"] for word in tokens]\ntest_sentence_lengths = [len(tokens) for tokens in test[\"tokens\"]]\nTEST_VOCAB = sorted(list(set(all_test_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\nprint(\"Max sentence length is %s\" % max(test_sentence_lengths))","execution_count":35,"outputs":[{"output_type":"stream","text":"29430 words total, with a vocabulary size of 8511\nMax sentence length is 22\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We then use the Keras Tokeniser to create a set of indices for each word. So since TRAINING_VOCAB represents the set of unique words, the length of this list gives the max number of indices required. So each tweet is now represented as a set of numbers with each word replaced by an index. So what the fit_on_texts method does is, it looks at the frequencies of words appearing in all the tweets and gives a lower index if its frequency is higher. So a word like 'injured' might occur most frequently in this data set and might be given the index 1. Now the texts_to_sequences method replaces each word in every tweet with the index it was fit with. Train_word_index is a dictionary containing the index mapped to the corresponding word. So its length gives set of unique words. So now we have lists of indices as tweets. But each tweet is of different length, so we pad with zeroes. Now each tweet is a sequence of indices and its length is 50."},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 50\n\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D,MaxPooling1D, Embedding\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\n\ntokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\ntokenizer.fit_on_texts(train[\"Text_Final\"].tolist())\ntraining_sequences = tokenizer.texts_to_sequences(train[\"Text_Final\"].tolist())\ntrain_word_index = tokenizer.word_index\nprint(\"Found %s unique tokens.\" % len(train_word_index))\ntrain_cnn_data = pad_sequences(training_sequences, \n                               maxlen=MAX_SEQUENCE_LENGTH)","execution_count":36,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"Found 14247 unique tokens.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Here, we also tokenise the test data into a sequence of indices as well and also pad the sequences. But the indices used are same as the ones used for the train data. And unseen words are relaced with 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(test[\"Text_Final\"].tolist())\ntest_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n# print(test_cnn_data.shape)\n","execution_count":37,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating word embeddings\nNow, we use a trained word2vec model. This model takes in a corpus of text as input, which in this case is the set of words in the tweets in the order in which they appear. This represents a continuous bag of words. It then creates vectors for each word. These vectors now constitute a 300 dimensional vector space with words sharing common context appearing closer together in the vector space. Creating word embeddings this way helps us create a representation for words in their linguistic contexts. So we load the path where the model is stored. We then load the model in."},{"metadata":{"trusted":true},"cell_type":"code","source":"# # import gensim.downloader as api\n# # path = api.load(\"word2vec-google-news-300\", return_path=True)\npath='../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin'\nfrom gensim import models\nword2vec = models.KeyedVectors.load_word2vec_format(path, binary=True)\n","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now construct vectors for each word such that if the word already exists in the word2vec model, the vector for that word is used, and if it is not, a random vector is used. So each vector length is 300. So train_embedding_weights contains the vectors for each word."},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM = 300\ntrain_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\nfor word,index in train_word_index.items():\n    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\nprint(train_embedding_weights.shape)","execution_count":39,"outputs":[{"output_type":"stream","text":"(14248, 300)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Convolutional Neural Network\nWe now define our CNN with an embedding layer. So any data that goes into this model is embedded through word2vec like we just did. So for test data this happens here, based on the train_embedding_matrix as weights, an embedding layer is created for the test input. This embedding layer is a layer of word2vec vectors of the test data. This layer acts as input to the convolutional layer. So for the convolutional layer is of depth 5. The 5 filters are of sizes 2,3,4,5,6. So the embedded input passes through each filter and a global max pooling layer that makes the number of parameters that pass through the subsequent layers smaller, by taking max parameter in a region. A relu activation function is also applied. The resultant is passed through a dropout layer with 10 percent dropout. Then a dense layer that retains 128 parameters and relu activation function is used and then another dropout layer. We end with a final dense layer and a relu activation layer that provides probabilities of 'disaster' and 'not disaster'. We then define loss function and the optimiser for backtracking and updating the weights. We return this model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n    \n    embedding_layer = Embedding(num_words,\n                            embedding_dim,\n                            weights=[embeddings],\n                            input_length=max_sequence_length,\n                            trainable=False)\n    \n    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n    embedded_sequences = embedding_layer(sequence_input)\n\n    convs = []\n    filter_sizes = [2,3,4,5,6]\n\n    for filter_size in filter_sizes:\n        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n        l_pool = GlobalMaxPooling1D()(l_conv)\n        convs.append(l_pool)\n\n\n    l_merge = concatenate(convs, axis=1)\n\n    x = Dropout(0.1)(l_merge)  \n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.2)(x)\n\n    \n    \n    preds = Dense(2, activation='sigmoid')(x)\n\n    model = Model(sequence_input, preds)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['acc'])\n#     model.summary()\n    return model","execution_count":40,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So here we assign the labels to y_train for training. We then also assign the padded tokenised tweets of training data to x_train."},{"metadata":{"trusted":true},"cell_type":"code","source":"label_names = ['Disaster', 'Not a Disaster']\ny_train = train[label_names].values\n\n\n# print(y_train)\nx_train = train_cnn_data","execution_count":41,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We define number of epochs and batch size for training.\nWe also define the early stopping criteria so training stops when validation loss reaches a minimum. This prevents overfitting."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nnum_epochs = 4 #3 is enough but just testing\nbatch_size = 24\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1)","execution_count":42,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We create an instance of our model and pass the training weights, max sequence length, no. of unique words, embedding dimension, and the no. of output labels to generate."},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in range(5):\n#     print('Trial-',i)\nmodel = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n                len(list(label_names)))\n\nhist = model.fit(x_train, y_train, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size,callbacks=[es])\n","execution_count":43,"outputs":[{"output_type":"stream","text":"Train on 6851 samples, validate on 762 samples\nEpoch 1/4\n6851/6851 [==============================] - 7s 989us/step - loss: 0.5366 - acc: 0.7343 - val_loss: 0.4557 - val_acc: 0.8084\nEpoch 2/4\n6851/6851 [==============================] - 2s 344us/step - loss: 0.4026 - acc: 0.8248 - val_loss: 0.4374 - val_acc: 0.8150\nEpoch 3/4\n6851/6851 [==============================] - 2s 362us/step - loss: 0.3193 - acc: 0.8678 - val_loss: 0.4556 - val_acc: 0.8091\nEpoch 00003: early stopping\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The model's predictions is stored. This is a list of probabilities for both 'disaster' and 'not a disaster'."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\nprint(predictions)","execution_count":44,"outputs":[{"output_type":"stream","text":"3263/3263 [==============================] - 0s 152us/step\n[[0.67312723 0.28790978]\n [0.9203591  0.05675971]\n [0.9824954  0.0150611 ]\n ...\n [0.77062774 0.17280972]\n [0.9108981  0.07327799]\n [0.44787818 0.5650634 ]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Next from the probability scores, we assign 1 if 'disaster' class is a higher probability and 0 otherwise."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = [1, 0]\nprediction_labels=[]\nfor p in predictions:\n    prediction_labels.append(labels[np.argmax(p)])\n# print(prediction_labels)\ni=1\n# for p in prediction_labels:\n#     print(i,'-',p)\n#     i+=1","execution_count":45,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now add the predictions in the dataframe. We also write the predictions into the submissions file."},{"metadata":{"trusted":true},"cell_type":"code","source":"test['target']=prediction_labels\n# print(test[['tokens','target']])\nsubmissions=pd.DataFrame(pd.read_csv('../input/nlp-getting-started/sample_submission.csv'))\n# submissions['target']=prediction_labels\n# print(submissions)\n# submissions.to_csv('/kaggle/working/submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some random code below pls ignore"},{"metadata":{"trusted":true},"cell_type":"code","source":"# submissions=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\n# comparewithnb=pd.DataFrame(pd.read_csv('../input/comparewithnb/filename11.csv'))\n# cwnb=comparewithnb['0'].to_list()\n# print(len(cwnb))\n# count=0\n# mismatch=[]\n# for i in range(3263):\n#     if(cwnb[i]==prediction_labels[i]):\n#         count+=1\n#     else:\n#         mismatch.append(i)\n# print(count)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testlabels=pd.DataFrame(pd.read_csv('../input/testlabels2/submission.csv'))\nlabels=testlabels['target'].to_list()\ncount=0\nmismatch=[]\nfor i in range(3263):\n    if(labels[i]==prediction_labels[i]):\n        count+=1\n    else:\n        mismatch.append(i)\nprint(count)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I've implemented the SVM, which tries to divide datapoints into classes by using a hyperplane. This hyperplane must be as distant from the two classes as possible. The support vectors are the points closest to the hyperplane."},{"metadata":{},"cell_type":"markdown","source":"An alternate form of counting and vectorising is using tf-idf, where the frequency of a word is compared with frequency across all documents, so its frequency can be attributed to being because of its existence in a class. A word that is more common across all documents will have a lower idf weight. The smooth_idf parameter is used to calculate weights as id there is another document that contains all words in the collection exactly once. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n\n##tfidf\ncount_vect = CountVectorizer()\n\nX_train_counts = count_vect.fit_transform(train[\"Text_Final\"])\nprint(X_train_counts.shape)\n\n\ntfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True).fit(X_train_counts)\nX_train_tf = tfidf_transformer.transform(X_train_counts)\nprint(X_train_tf.shape)","execution_count":25,"outputs":[{"output_type":"stream","text":"(7613, 14224)\n(7613, 14224)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_counts = count_vect.transform(test['Text_Final'])\nX_test_tf = tfidf_transformer.transform(X_test_counts)\nprint(X_test_counts.shape)\nprint(X_test_tf.shape)","execution_count":26,"outputs":[{"output_type":"stream","text":"(3263, 14224)\n(3263, 14224)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_idf = pd.DataFrame(tfidf_transformer.idf_, index=count_vect.get_feature_names(),columns=[\"idf_weights\"]) \ndf_idf.sort_values(by=['idf_weights'])\n# df_idf","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"         idf_weights\nget         3.909465\nlike        4.013488\nfire        4.135626\ngo          4.201172\namp         4.227317\n...              ...\nchoppas     9.244597\nloser       9.244597\nloses       9.244597\nlorr        9.244597\nzzzz        9.244597\n\n[14224 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idf_weights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>get</th>\n      <td>3.909465</td>\n    </tr>\n    <tr>\n      <th>like</th>\n      <td>4.013488</td>\n    </tr>\n    <tr>\n      <th>fire</th>\n      <td>4.135626</td>\n    </tr>\n    <tr>\n      <th>go</th>\n      <td>4.201172</td>\n    </tr>\n    <tr>\n      <th>amp</th>\n      <td>4.227317</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>choppas</th>\n      <td>9.244597</td>\n    </tr>\n    <tr>\n      <th>loser</th>\n      <td>9.244597</td>\n    </tr>\n    <tr>\n      <th>loses</th>\n      <td>9.244597</td>\n    </tr>\n    <tr>\n      <th>lorr</th>\n      <td>9.244597</td>\n    </tr>\n    <tr>\n      <th>zzzz</th>\n      <td>9.244597</td>\n    </tr>\n  </tbody>\n</table>\n<p>14224 rows Ã— 1 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nfrom sklearn import svm,metrics\n\ntrain_model=svm.SVC().fit(X_train_tf, train[\"target\"].values)\npredictions=train_model.predict(X_test_tf)\n\n# SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n# hist2=SVM.fit(X_train_tf,train_y)\n# predictions_SVM = SVM.predict(X_test_tf)\n","execution_count":28,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVM does seem to do better than CNNs, but I will look into this later."},{"metadata":{"trusted":true},"cell_type":"code","source":"##SVM\ncount=0\ntestlabels=pd.DataFrame(pd.read_csv('../input/testlabels2/submission.csv'))\nlabels=testlabels['target'].to_list()\nmismatch=[]\nfor i in range(3263):\n    if(labels[i]==predictions[i]):\n        count+=1\n    else:\n        mismatch.append(i)\nprint(count)\nsubmissions['target']=predictions\n\nsubmissions.to_csv('/kaggle/working/submission.csv',index=False)\n\n","execution_count":31,"outputs":[{"output_type":"stream","text":"2611\n","name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'submissions' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-8d50444f231e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmismatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msubmissions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msubmissions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'submissions' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"2611/3623","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"0.7206734750207011"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}